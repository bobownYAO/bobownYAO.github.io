[{"title":"Briefing","url":"/2026/01/07/briefing/","content":"\n这应该是这个站的第一篇正式的post，希望不是最后一篇\n\n\n2026年的开始，利用在上海实习摸鱼的一段时间，我抽出一些上午和下午，还有最清醒的夜晚，创建了这个网站。我其实没有什么想发布的，这个网站本身就不以功能性所驱动，也许以后会传一些wyy的歌单，希望你能来听\n这个site可以说是一种leisure的产物，有点像一个涂鸦空间，以前洗澡时胡思乱想，或者是走在路上产生很多新奇的想法，都在时间的缝隙流沙般的去了，所以偶尔也想抓住一点，写下来一点，留下一些痕迹……因此大概率也不会发什么正经玩意，希望你在这里玩的开心，我正在努力装点中ing\n\n这个网站可以说是我和Gemini边聊边搭的，并没有参考任何现成的视频教学，使用了Hexo的框架和Redefine-theme。相比浏览这个网站，也许搭建这个网站才是我更大的兴趣点所在，html&#x2F;css&#x2F;markdown（但是markdown真的比LaTeX简单好多qwq），很多东西都是from scratch的状态，不过最终发现学的挺爽的，这大概反映了我这两年的学习态度\n\n以下是一些吐槽，不听也罢\n如果换做是在两年前，笔者也许完全不会预料到，两年后会有这样一个网站，作为我学习与生活的一些记录。那是一个对于“内卷”唯恐避之而不及的年代，所有人都努力把自己包装成“摆”的样式，然后实践着他人口中的“卷”。所以说，当时的我，也许会陷入“不想让别人知道我在内卷”的自我认知矛盾中吧\n我不认为那是一种健康的环境，但是我当时决定要永久的悬置这个难题，因为我既不能理解，也不能接受为什么人们总是要言说“卷”或者“摆”，或者是“偷偷卷”亦或“偷偷摆”。我一直以为学习是一个很simple的东西，但是浸染在这种环境中，不免让人有些沮丧 \n所以笔者就很不感冒那些把“卷”“摆”挂在嘴边的人。说实话，想学就可以一直学习，不想学就是累了就开始休息，很chill，很干爽，很正常，亦无他，但是非要说某一种状态是“卷”还是“摆”，称自己为“摆”，指认别人为“卷”，真的是非常boring的行为，而且hypocritical。\n笔者写到这里觉得真的是太中二了，遂添加很多的歌词作为填充物（有什么用吗我请问了）：\nさよならの速さで顔を上げて\nいつかやっと夜が明けたら\nもう目を覚まして。見て。\n寝ぼけまなこの君を何度だって描いているから\n傘を出してやっと外に出てみようと決めたはいいけど、靴を捨てたんだっけ\n裸足のままなんて度胸もある訳がないや\nどうでもいいかな\n读了一整年高三，我大概是理解了为什么有这样一种对于他人的学习的排斥心理，所以我打算离开这种环境，很多读作“大一”，写作“高四”，我打算离开这种环境，到一个我可以随心所欲学自己想要的东西的地方，不存在的是push，而我只是想learn moreThat’s all 谢谢你听我絮叨\n","tags":["Life"]},{"title":"Notes on Section 2.1-2.2","url":"/2025/11/23/Notes-on-Section-2-1-2-2/","content":"首先这里想讲的是笔者一开始接触环境的一些理解误区和模糊首先Anaconda可以理解为一个环境的分隔工具，可以在同一个设备上创建多个子环境，这样就可以允许在同一设备的不同文件夹中分别使用python的不同版本和相应库\n然后pytorch是一个库\n张量（tensor）本质上是一个多维向量\nTensor在规定上的有标准运算，适用于相同结构的张量，具体内容是元素对元素一对一操作，这个和矩阵的加减是同理的，但是tensor在乘除和幂次上也是体现为一对一\nCat（concatenate）可以给定轴的连接tensor，当然这个连接不会影响完整性的前提下\n广播机制是另外一种tensor合并涉及的步骤，这个也是笔者一开始比较困惑的一点\n其步骤可以简略理解为以下\n首先有张量a，b，维度表示分别为x，y\n此时将最高维度对其，低维位置为空则补为1\nX&#x3D;（3，5，1）\nY&#x3D; （1，6）\n（3，5，1）\n（1，1，6）\n然后进行拓展\n要求对应位置上下两数相等或者存在1，否则是无法广播到可以相加的形式的，也即报错\n（3，5，6）\n（3，5，6）\n分别广播拓展之后相加\n得出最终结果\n诡异：逻辑运算符可以构建tensor\ne.g. c&#x3D;a&#x3D;&#x3D;b得到c这样一个按位的布尔矩阵\n","categories":["Notes","d2l"],"tags":["Study"]},{"title":"first_page","url":"/2026/01/06/first-page/","content":"this is a page generated by a person who is building the site.\n"},{"title":"Notes on Section 2.3","url":"/2025/11/24/Notes-on-Section-2-3/","content":"这一部分，主要说的就是线性代数，其实没什么好说的，不过还是有一些对于深度学习的特化（额也有可能是我还没学到的线代部分）\n首先一个点是向量的长度和维度是一个概念（毕竟是一维的，如果要从结构上使向量的维度不平凡好像就是这样定义）\n降维操作\n这个主要是通过一些统计相关的函数实现，比如求和，求平均数等函数，这些函数内置于pandas&#x2F;numpy\n需要注意的是！！！默认调用这些降维函数时确实会发生降维，比如\nx &#x3D; np.arange(4)\nx, x.sum()\n表示为 (array([0., 1., 2., 3.]), array(6.))\n其中我们发现6.已经是一个标量了\n当然我们可以通过一些操作保留维度（那么最小的那个维度对应的组里面都是单独的元素）\nsum_A &#x3D; A.sum(axis&#x3D;1, keepdims&#x3D;True)\n既这个keepdims 输出效果如下\narray([[ 6.],\n[22.],\n[38.],\n[54.],\n[70.]])\n点积\n这个就是矩阵的乘法，向量-向量&#x2F;矩阵-向量同理，不过需要注意的是两个向量x,y的乘法实际是xTy，因为本部分默认所有的向量都是列向量的形式\n另外就是Hadamard积，那个是元素对元素的积，应该和上述的积加以区分\n范数(norm)，是一个常用的运算符。范数体现的是一个向量整体性分量的大小，\n$$||\\mathbf{x}||_p &#x3D; \\left( \\sum_{i&#x3D;1}^n |x_i|^p \\right)^{1&#x2F;p}$$\n\n我们可以想象一个n维空间，那么一个长度为n的向量就代表了这个空间中的一个坐标，这时候我们想要计算原点到这个坐标的路径长度（当然这个路径的模式和范数L下标有关）\n我们有L1是曼哈顿距离\nL2是欧几里得距离\nL3及以上，我们可以理解为是越长的那个分量约占据主导（不过这个范数本身还是线性的）\n因为范数运算将整个向量完全降维为标量，所以我们可以比较不同维度向量的范数，这是可行的\n\n\n","categories":["Notes","d2l"],"tags":["Study"]},{"title":"Notes on Section 2.4","url":"/2025/11/25/Notes-on-Section-2-4/","content":"在本书中d&#x2F;dx和D都表示微分运算符，即f’(x)&#x3D;Df(x)&#x3D;df(x)&#x2F;dx\n关于偏导数的一些关系\n\n\n关于梯度这里几个性质的证明如下：\n其实这四条基本上的思路是相同的\n\n\n注意这里numerator layout的具体排列形式，可以理解为先把y向量视为一个行向量，再沿着列方向将其以对应的x展开（因为一共有n项x，故展开了n行），并两者做偏微分\n","categories":["Notes","d2l"],"tags":["Study"]},{"title":"Notes on Section 2.5","url":"/2025/11/28/Notes-on-Section-2-5/","content":"首先我们应该明确梯度在深度学习中的应用\n\n\n我们理解了梯度的一些概念\n下面我们可以开始看一下反向传播，反向传播本身是计算梯度的一个算法\n\n\n\n\n上面说的是具体的实现，当然理解上说，反向传播比较像一个递归结构，也就是我下面的一些理解\n也就是说，对于一个复合的复杂结构函数，我们像剥洋葱一样将其向下分解\n反例就是我直接用变量参数去对函数偏导，这样不仅麻烦，再参数很多的时候会有计算浪费（因为很多的式子对于不同参数的偏导可能是一样的）\n\n\n","categories":["Notes","d2l"],"tags":["Study"]},{"title":"Notes on Section 2.6","url":"/2025/11/28/Notes-on-Section-2-6/","content":"基本的概率入门\n\n这个叫边际化是因为相当于消去了一个参数的影响\n","categories":["Notes","d2l"],"tags":["Study"]},{"title":"Notes on Section 3.1","url":"/2025/11/29/Notes-on-Section-3-1/","content":"\n所以核心思想就是找到最佳权重使得偏移量最小\n然后这个线性方程组的拟合是可以有解析解的\n\n但是这个用处不是很大（当然知道如何推导这个还是比较有意义的），因为大部分拟合函数都不是线性的，参数对输出结果的影响也不一定是线性的\n所以这里引入了一个随机梯度下降（gradient descent）的概念，可以在无法得到有效解析解的情况下训练模型\n“梯度下降最简单的用法是计算损失函数（数据集中所有样本的损失均值） 关于模型参数的导数（在这里也可以称为梯度）。 但实际中的执行可能会非常慢：因为在每一次更新参数之前，我们必须遍历整个数据集。 因此，我们通常会在每次需要计算更新的时候随机抽取一小批样本， 这种变体叫做小批量随机梯度下降（minibatch stochastic gradient descent）。”\n注意，这里的随机抽取的对象是样本，也就是说，我们有很多的训练数据，但是我们不会同一时间全部使用他们，而是在一轮训练中只抽样式的选择一些数据点。但是，对于参数来说，是的，我们会计算整体梯度并更新每一个参数。\n以下是一个简要的比较\n\n\n另外补充以下符号的含义，权当复习了（其实就是主播忘记了qwq\n对于w，b两个参数分别的更新，可以更为详细的写为以下\n\n这个解析式也可以推一下，挺有意思的\n给一个w的范例，另一个是同理的\n\n3.1.2矢量化加速\n这个就是一个概念性的东西（基于numpy库的一些优化）\n\n反正就是用numpy的张量计算比for循环要快（还有比python的循环慢的东西吗。。。）\n3.1.3正态分布与平方损失\n首先我们有正态分布的表达式如下\n\n\n这里有有一个非常有意思的现象\n就是虽然我们是用np.arange创建的x\n但是在pytorch框架下，这个参数很可能被认为是一个张量\n于是当使用绘图函数（一般接受的是numpy数组输入）的时候，我们需要指定并转换，这也就是其中asnumpy的作用\n\n这里稍微提一嘴这个，这个也就是我们修正函数的依据（也就是为什么最小化均方差损失是正确的函数拟合方向）\n\n这里的两个观点都是对的哦，我觉得对于增进我的理解比较有帮助\n\n这个没啥好说的，摆烂了\n\n“虽然可能受到鸟类的启发，但几个世纪以来，鸟类学并不是航空创新的主要驱动力。 同样地，如今在深度学习中的灵感同样或更多地来自数学、统计学和计算机科学。”\n","categories":["Notes","d2l"],"tags":["Study"]},{"title":"Notes on Section 3.2","url":"/2025/12/01/Notes-on-Section-3-2/","content":"“在这一节中，我们将从零开始实现整个方法， 包括数据流水线、模型、损失函数和小批量随机梯度下降优化器。 虽然现代的深度学习框架几乎可以自动化地进行所有这些工作，但从零开始实现可以确保我们真正知道自己在做什么。”\n首先是生成数据，这里的X也就是样本空间其实也是正态分布随机生成的，所以其实每次调用的时候都不一样（当然实际使用的时候肯定是以一个固定的数据集）\nY也就是我们处理得到的输入了（注意这里使用了y.reshape((-1, 1))，-1其实是占位符，pytorch会自动计算这一部分的大小，这一步的操作是保证y是一个列向量的形式）\n\n同时我们可以实现一下可视化（然后注意到d2l是深度整合matplotlib的）\n\n这边同时解释一些操作函数\n\n那么我们接下来就可以开始读取数据了\n这里我们根据随机梯度下降的算法思想，打乱样本并且分批次选取\n\n注意到这里实际上遍历了每个样本点（相当于打乱样本点后用batchsize分划打包，然后一份份给出去算梯度）\n然后我们注意到这个yield指令，这个是有别于return的，直观上看，其在执行后不会关闭函数\n\n一个包含 yield 的函数在被调用时，会创建一个特殊的本地对象，即生成器对象（Generator Object），这个对象内部保存了函数执行的完整状态和上下文，包括上一次的断点。\n数据处理和选取这块结束之后，我们还需要初始化一下权重（w，b）\n\n接下来我们分别定义模型（也就是参数和权重的作用关系）&#x2F;损失函数（一般就是平方损失函数）&#x2F;优化算法\n这里我打算学习一下两个pytorch计算梯度的重要组成，也就是.grad属性 和 .backward()函数\n\n这个累加性是值得注意的，这解释了为什么梯度提取之后需要清零\n假设 w 已经被初始化并设置 requires_grad&#x3D;True在反向传播之前print(w.grad) # 输出: None\n# 经过 L.backward() 之后\nprint(w.grad) # 输出: tensor([[-0.1234], [0.5678]]) （这是计算得到的梯度值）\n在优化器中，使用后必须清零w.grad.zero_()\nprint(w.grad) # 输出: tensor([[0.], [0.]]) 非空但是为0向量\n以下则是关于.backward()\n\n这里的L即我们要求的损失函数，其是变量和权重等等参数复合成的复杂函数。Pytorch在自动积分的时候，会构建DAG理解这样一个函数，其中被指明需要计算梯度的变量也会被计算（然后就可以用.grad 读取了）\n","categories":["Notes","d2l"],"tags":["Study"]},{"title":"Notes on Section 3.4","url":"/2025/12/04/Notes-on-Section-3-4/","content":"我们可以发现，一个回归模型可以用于预测量的问题，但是我们同样也想关注分类问题\n\n所以其实是一个概率模型\n对于一个分类问题，分类的类别往往是等价的（或者至少是没有量化可比性的，这时候我们需要引入新的标识系统来处理数据结构）\n独热编码（one-hot encoding）就是这样一个例子\n具体实现也很简单，对于N个元素的分类组，构建一个N维向量空间，则每个单独元素都是其中的一个基向量，且两两不同。这维持了范数的恒定，也保证线性无关\n\n这里需要解释的是，首先输出层是由多个数据组成的，分别代表了机器在当先权重下对每个分类的预测的可能度，所以这些输出应是不相关的，其和也不符合概率的加和为100%的规律，而是只是由数值的大小来决定是这个类别的相对可能性\n\n为了解决上述的一个非归一化的问题，我们引入softmax函数\n\n显然，上面的函数变换达到了要求，也没有改变各个输出之间的大小关系\n然后我们可以取一定批量的样本然后矢量化，相当于是打包操作，这个操作主要是利用GPU多线程的特性来提高效率\n\n损失函数还是同理使用最大似然估计\n\n这里的意思就是求这个模型准确预测这个批次中所有样本的概率，当然这个概率在样本数量大的时候会很小，所以我们接下来引入其负对数进行处理\n\n\n根据独热编码的特性，我们可以给出损失函数的等价变形，也称为\n交叉熵损失（cross-entropy loss）\n\n以下给出了等价推导\n\n经过代数变换，我们可以得到以下简化公式（下标指的是分类标签）\n\n这个的导数相对是容易的\n\n然后进一步回溯到W&#x2F;b就可以得到需要的梯度了\n然后我们关注一下信息熵部分的内容（香农熵）\n\n我们这里涉及和交叉熵和香农熵本质联系\n\n在训练单个样本的时候，香农熵是0（因为只有一个正确答案），所以说最小化交叉熵和交叉熵向香农熵趋近是等效的\n","categories":["Notes","d2l"],"tags":["Study"]},{"title":"Notes on Section 3.3","url":"/2025/12/03/Notes-on-Section-3-3/","content":"我们可以使用d2l内置的函数生成样本，注意这个函数生成样本点是自动含有噪音的\n详情可以参考下面的def\n\n生成数据之后我们创建一个迭代器来处理我们的数据\n\n这里面涉及了两个pytorch函数\n\n\n这里生成的data_iter虽然是一个迭代器，可以用for&#x2F;in结构读取，但是本身不是一个python的基础迭代器类型，可以通过iter（data_iter）转换成基础迭代器的（比如list&#x2F;tuple&#x2F;str&#x2F;range这些可迭代对象，在使用for in 的时候程序也是会默认转换成iter类型再执行循环的）\n深度学习的模型构建可以使用内建的框架\n主要由nn（neuron network）这个库实现\n\n也就是sequential包含linear等等计算层，形成一个计算链\n接下来就可以初始化参数了\n\n注意到weight和bias是linear层的内置参数名，所以说这里就是指定的调用\n\n接下来我们分别定义损失函数和优化算法即可\n\n这里的loss类型就是平方差即L2范数\n\nSDG这里指的是随机梯度下降算法\n\n这里的学习率也就是我们在梯度优化公式中看到的步长\n\n\n这里就是一个训练过程（总计三步）\n其中.step()步整合了梯度的提取和参数的更新，也就是完成了优化的流程，不需要再手动赋值给参数（但是前面的backward计算梯度还是要写的，因为这个不在优化算法打包的范围内，同时记得梯度清零）\n","categories":["Notes","d2l"],"tags":["Study"]},{"title":"Notes on Section 3.5","url":"/2025/12/05/Notes-on-Section-3-5/","content":"\n这两个数据集都是基准训练集，包含了28*28的灰度图像万余个，图像有一个特定主题（比如是某一个数字，或者是某一种衣服）\n在前置库处理之后，我们导入需要训练的数据库\n\n数据集中的数据结构的解释如下\n\n\n另外我们需要注意到的是，这个训练集的外层框架是一个二维组，\n目的在于绑定图片和其标签（tensor，int）结构的元组（数字标签）\n\n","categories":["Notes","d2l"],"tags":["Study"]},{"title":"Notes on Section 3.6","url":"/2025/12/07/Notes-on-Section-3-6/","content":"这个单元旨在空头构建一个基于mnist的训练模型\n首先我们引入训练集\n（这里有一个已经打包的程序，可以直接使用，batch_size指定了同时的训练量）\n\n这里的784对应的是每个像素点的输入，10个输出对应10个种类的概率情况\n\n然后我们常规的使用正态分布初始化权重，bias初设为0\n然后我们的softmax函数部分\n\n这里解释一下这个sum的原理\n\n需要注意的是，当我们不使用keepdim时，被指定的维度会消失，也就是生成的张量维度比原张量少1，但是使用keepdim则保留了指定的维度\n实现效果如下\n\n然后定义模型\n\n这里我们将X展平成一个二维向量，相当于就是将图像的二维数据一维化了，和W.shape[0]的输入格式相匹配，然后执行矩阵乘法，然后添加权重\n\n这里定义的损失函数，如果不是很懂公式可以往回看，这里使用的是简化后的公式\n返回值是这个批次的交叉熵，一个一维向量，每个元素对应一个样本点的交叉熵\n但是一个样本点的预测和实际值是否相符合是一个布尔值，所以我们可以通过比较给出权重下的准确度\n\n同样，对于一个打包好的net，我们同样也可以估计其精度\n\n这里有一个调为eval的模式\n另外我们注意到关闭了梯度计算，是为了节省算力（因为不需要反向传播）\n另外作者提到\n“我们在(Accumulator实例中创建了2个变量， 分别用于存储正确预测的数量和预测的总数量)。”\n以下是实现形式\n\n然后我们编写训练的部分\n这里的isinstance就是为了判断损失函数和当前模型是否符合要求（if else兼容了两种处理loss的方式）\n解释一下为什么有分支\n\n\n中间作者引入了一个可视化模块Animator，这里不展开\n\n最后的assert是一个断言语句，检验模型是否达到要求\n","categories":["Notes","d2l"],"tags":["Study"]},{"title":"Notes on Section 3.7","url":"/2025/12/09/Notes-on-Section-3-7/","content":"这一节主要是简要的实现softmax\n库初始化\n\n神经网络层初始化和权重初始化（注意到只有该层是线性层Linear时我们才应用权重的正态分布初始化）\n这里一共有两层，一层是用来解包展平数据的也就是flatten层\n\n这些名称是指明的，需要准确使用\n我们注意到softmax函数使用了指数来规范化，但这个可能导致数据溢出的问题，所以我们可以做如下优化\nSoftmax\n\nLoss（因为需要取对数）\n\n合并了softmax函数和loss函数的计算，达到了优化的效果\n完整训练代码如下\n\n","categories":["Notes","d2l"],"tags":["Study"]},{"title":"Notes on Section 4.1","url":"/2025/12/20/Notes-on-Section-4-1/","content":"线性模型不总是正确的，因为并不是所有的结果都可以用其元素的值单调性的表示（比如一张照片更加像猫或者是狗，这个在我们把照片倒置之后，线性模型就会出现问题）\n“我们可以通过在网络中加入一个或多个隐藏层来克服线性模型的限制。 这种架构通常称为多层感知机（multilayer perceptron），通常缩写为MLP。”\n这里面新增加的层叫做“隐藏层”\n\n“激活函数的输出（例如，）被称为活性值（activations）。 一般来说，有了激活函数，就不可能再将我们的多层感知机退化成线性模型”\n激活函数，可以理解为一个非线性函数，应用在输入的所有数据上，其一般是不含权重的，所以在训练过程中不会改变。\nReLU函数\n\n“当输入为负时，ReLU函数的导数为0，而当输入为正时，ReLU函数的导数为1。 注意，当输入值精确等于0时，ReLU函数不可导。”\n使用导数来定义这个函数，而不是使用逻辑判断式，是因为我们依然要使用反向传播计算梯度，不能简单的掐断传递链。\n\n\n“使用ReLU的原因是，它求导表现得特别好：要么让参数消失，要么让参数通过。 这使得优化表现得更好，并且ReLU减轻了困扰以往神经网络的梯度消失问题”\nSigmoid函数\n\n可以观察到，这个函数将任意实数域内的一个数压缩成（0，1）内的一个数，所以这个函数也叫（squashing function）\n\nTanh函数\n双曲正切函数将其输入压缩转换到区间(-1, 1)\n\n\n","categories":["Notes","d2l"],"tags":["Study"]},{"title":"Notes on Section 4.4","url":"/2025/12/25/Notes-on-Section-4-4/","content":"机器学习的目标是发现“模式”，但是我们要求这个模式是general的\n“将模型在训练数据上拟合的比在潜在分布中更接近的现象称为过拟合（overfitting）， 用于对抗过拟合的技术称为正则化（regularization）”\n\n\n独立同分布假设是指对于任意两个测试样本点，这两个样本点是互不干扰的，所以说也会在同一个逻辑框架下给出回答\n但是事实上很多事物不符合或者轻微违反独立同分布假设，最典型的是具有时间依赖性&#x2F;周期依赖性的一些事件\n\n“在机器学习中，我们通常在评估几个候选模型后选择最终的模型。 这个过程叫做模型选择。 有时，需要进行比较的模型在本质上是完全不同的（比如，决策树与线性模型）。 又有时，我们需要比较不同的超参数设置下的同一类模型。”\n“当训练数据稀缺时，我们甚至可能无法提供足够的数据来构成一个合适的验证集。 这个问题的一个流行的解决方案是采用折交叉验证。 这里，原始训练数据被分成个不重叠的子集。 然后执行次模型训练和验证，每次在个子集上进行训练， 并在剩余的一个子集（在该轮中没有用于训练的子集）上进行验证。 最后，通过对次实验的结果取平均来估计训练和验证误差。”\n\n也即我们需要找到泛化损失最低点\n","categories":["Notes","d2l"],"tags":["Study"]},{"title":"Notes on Section 4.2-4.3","url":"/2025/12/24/Notes-on-Section-4-2-4-3/","content":"多层感知机MLP from scratch\n我们沿用上面线性模型预测mnist数据集的框架，并在其上增加非线性层\n\n这里用一个相同形状的零张量a来实现ReLU函数\n搭建和线性层相近整体实现如下：\n\n同样我们可以使用API快速实现这个模型\n\n","categories":["Notes","d2l"],"tags":["Study"]},{"title":"Notes on Section 4.6","url":"/2025/12/27/Notes-on-Section-4-6/","content":"暂退法（dropout）\n在4.5中，我们使用L2正则化，这里我们引入暂退法，其本质思想是一致的，也就是防止某一些权重过分大，从而导致在这些方向上受噪音影响大。\n传统的理论认为，为了达到模型的泛化特性，需要使用更加简单的模型，也即更小的维度，另外一个角度是模型的平滑性，也就是在小的扰动下不会产生过于显著的晃动。\n“暂退法在前向传播过程中，计算每一内部层的同时注入噪声，这已经成为训练神经网络的常用技术。 这种方法之所以被称为暂退法，因为我们从表面上看是在训练过程中丢弃（drop out）一些神经元。 在整个训练过程的每一次迭代中，标准暂退法包括在计算下一层之前将当前层中的一些节点置零。”\n\n这里我们可以观察到，在舍弃之后，我们对剩余的张量进行了缩放\n需要注意的是\n\n这个算法的实现逻辑是，在单层开始前，我们均匀的从[0，1]中抽取和这层输出相同数量的样本，然后保留那些大于p的样本而舍弃其余。\n\n事实上dropout和ReLU函数往往是复合使用的\n\n在这个例子中，使用了两次ReLU和dropout\n同样的dropout也是一个已有的模板层，可以简洁实现\n\n","categories":["Notes","d2l"],"tags":["Study"]},{"title":"Notes on Section 4.5","url":"/2025/12/25/Notes-on-Section-4-5/","content":"权重衰减，也通常被称为L2正则化\n在4.4中我们会发现参数过多，过于复杂的参数组会导致过拟合。所以这自然引出了我们要控制权重复杂度的想法，而首先我们就应该获得当前权重的复杂度。这里的方法就是通过函数与零的距离来衡量函数的复杂度\n\n这里的操作方法也非常直观，就是在计算损失函数的时候为复杂权重增加一个“惩罚项”\n这里有必要说明一下为什么使用L2正则化\n\n\n可以看出，L2正则化本身更加的泛化\n最终的梯度下降更新公式\n\n实现方面只要在基础的线性层上更改就可以\n\n\n","categories":["Notes","d2l"],"tags":["Study"]},{"title":"Notes on Section 4.7","url":"/2025/12/28/Notes-on-Section-4-7/","content":"这一章节主要是阐述了，反向传播的计算原理，对梯度推导进行了展开陈述\n“因此，在训练神经网络时，在初始化模型参数后， 我们交替使用前向传播和反向传播，利用反向传播给出的梯度来更新模型参数。 注意，反向传播重复利用前向传播中存储的中间值，以避免重复计算。 带来的影响之一是我们需要保留中间值，直到反向传播完成。 这也是训练比单纯的预测需要更多的内存（显存）的原因之一。 此外，这些中间值的大小与网络层的数量和批量的大小大致成正比。 因此，使用更大的批量来训练更深层次的网络更容易导致内存不足（out of memory）错误。”\n","categories":["Notes","d2l"],"tags":["Study"]},{"title":"Notes on Section 4.8","url":"/2026/01/04/Notes-on-Section-4-8/","content":"这节研究的是初始化模型参数方面的考量\n糟糕的初始化选择会导致一些问题，比如“梯度消失”和“梯度爆炸”\n\n首先我们可以把每一层简化成一个变换，这个变换包括了常规的权重矩阵&#x2F;偏置&#x2F;非线性函数\n\n这样我们就可以利用链式求导法则得到这个的表达式了\n\n但是我们设想这样一种情况，也就是有多个线性层堆叠起来，这样的话我们的权重矩阵重叠起来，就很有可能被指数级放大或者缩小，以及ReLU等非线性函数的处理，也许会出现很小的值，这会导致梯度爆炸或者梯度消失\n\n这里我们引入另外一个概念，也就是线性层的对称性这样一个性质\n其主要阐述的就是，对于两个堆叠在一起的线性层，我们交换两个单元并且调整对应的权重结构，这样不会改变模型的效果，也就是，两个线性层本质是相同的（这从多个线性层本质上可以复合成一个线性层可以看出来）\n事实上，训练模型的时候，多个线性层的表现往往差于单层，不仅因为其耗费了大量的内存，还有多层迭代的不稳定性和放缩特性（事实上非线性函数在同样权重的表现下也不会产生不同的梯度，由于其本质还是非随机的）\n但是引入dropout则可以强行打破这个对称性，通过随机的丢弃一些神经元，这样的话每一层就会往不同的方向发展\nXavier初始化 可以配合以下链接\n深度学习中的参数初始化方法：Xavier详细推导+pytorch实现 - 知乎\n\n（配合这个食用就看的懂了）\n\n因为每个输出对应的权重都是独立的，所以说向量协方差为0，可以拆开，这里我们得到了输出向量组元素的期望是0，然后方差也可以相同方式的展开\n这个算法保证了输出向量组元素的方差等于输出向量的方差\n\n","categories":["Notes","d2l"],"tags":["Study"]},{"title":"Notes on Section 4.9","url":"/2025/12/29/Notes-on-Section-4-9/","content":"训练模型的数据分布和应用环境的数据分布的不同很可能导致模型完全不同的表现。\n同时，我们需要考量到模型本身对环境的影响，当模型在环境中进行决策时，环境可能会跟随决策而做出相应的变化，但是由于模型和环境是不交互的，所以这导致模型运行时的偏差甚至错误。\n分布偏移（distribution shift）是我们此处需要考虑的问题，也就是训练时的数据和测试&#x2F;实际应用时的数据结构和特征改变的这样一个可能\n当然在偏移满足一定条件的时候，我们还是可以通过模型得到合理的答案\n协变量偏移\n\n条件分布P（y | X）保持不变\n我们通俗的理解一下：我们处理问题的思路是（需求要素-逻辑链-结论），但是模型并不这样理解问题，事实上，模型并不会真正的理解哪些是真正的需求要素，而是相对单纯的对比相似性，这导致那些人类处理问题时往往会忽略的内容成为了模型在环境迁移时的阻碍（比如判断红绿灯时，天气的因素导致背景的色块完全不同，这可能影响模型的判断）\n标签偏移\n指的就是测试集中不同结果的分布和训练的数据不同\n从独立性的角度看，模型其实不会产生非常大的偏差，但是如果我们在训练模型的时候也使用失衡的样本，那么模型处理在边界处的输出就不可避免地产生偏倚（比如将模糊的照片大部分分类为猫因为在训练集中有90%的样本是猫）\n概念偏移\n“这听起来很奇怪——一只猫就是一只猫，不是吗？ 然而，其他类别会随着不同时间的用法而发生变化。”\n变化的 不变的 example\n\n\n基于这些可能的问题，我们提出分布偏移纠正&#x2F;标签偏移纠正来解决这些问题\n首先我们从上面可以看出两个偏差对应的变化和不变的关系\n所以说我们可以分别在训练的时候增加对应的权重，这个权重的大小意味着当前参数在真实环境中的相对重要性&#x2F;出现频率，所以我们也是正相关的放大其损失函数\n\n这里取对数计算本质是简化计算，只需要明白我们还是在估算权重beta的值\n因为训练和测试的输入数据都是有的，所以说这个数值是可以计算的\n\n同样的我们对于标签偏移也是这样解决，不过我们似乎没有办法获取目标分布\n（这是一个问题！）\n\n但是我们也有办法对这个值进行估算，首先我们可以用带有标签的训练集得到这个函数的混淆矩阵（也可以拿一个更加好的模型对拍）\n混淆矩阵C是一个k*k矩阵， 其中每列对应于标签类别，每行对应于模型的预测类别。 每个单元格的值c_ij是验证集中，真实标签为j， 而我们的模型预测为i的样本数量所占的比例。\n以及\n\n混淆参数*实际值得到的就是我们模型的预测输出值，这样的话通过矩阵的逆运算我们就可以得到p(y)，也就得到了我们需要的权重beta\n对于概念偏移纠正，这个不属于数据上的问题，所以很难用原则性的方式解决\n本章节的剩余部分介绍了不同的训练方式\n批量学习\n在线学习（交互式迭代）\n控制\n强化学习\n","categories":["Notes","d2l"],"tags":["Study"]},{"title":"Notes on Section 5.1","url":"/2026/01/03/Notes-on-Section-5-1/","content":"在神经网络中，层一般指的是“（1）接受一组输入， （2）生成相应的输出， （3）由一组可调整参数描述。”的这样一个整体\n我们可以将多个层整合，称为“块”，块之间也可以继续连接形成更大更复杂的块，事实上这一生成过程往往是递归的\n一个块需要具有以下功能\n\n1就是说需要一个输入，2就是说会给出一个输出，345就是指会存所有需要的东西\n\n我们可以使用这样一个class块来实现简洁的块功能，但是这个块是固定的\n（nn.Module&#x2F;super都是用来在块内部使用nn库的函数而设计的）\n所以我们可以用同样的思路写一个自定义块\n\n注意的是这里的_modules是一个特定的写法，这样子pytorch才能正确识别我们写入的层并正确执行相关的流程\n\n于是我们也可以给层加入一些固定参数（比如一个不被反向传播更新参数的固定层，和最后控制L1范数而不断折半的while循环，这个while循环并不会影响反向传播的运作）\n然后，最后我们可以把这些自定义的块组装在Sequential里\n\n","categories":["Notes","d2l"],"tags":["Study"]},{"title":"Notes on Section 5.2","url":"/2026/01/04/Notes-on-Section-5-2/","content":"本节，我们将介绍以下内容：\n\n访问参数，用于调试、诊断和可视化；\n参数初始化；\n在不同模型组件间共享参数。\n\n首先我们需要知道，在nn库中，虽然可能存在多个架构相同的层，但是每一层都有独立编码，这也就是说对应的参数都是可查询的，我们可以利用以下方法打印这些数据\n\n注意到的是，对于层内存储的weight&#x2F;bias等，属于复合参数实例，其中除了本身的数值以外，还有梯度信息和梯度激活信息\n\n我们也可以利用正则表达式一次性遍历所有的底层参数\n\n这里我们会注意到，从不同地址触发查找相同位置的参数，所返回的名称是有别的\n这说明这是一个树状的连接方式，参考下图\n\n这也就解释了以下两个实际上是指向等效位置的（另外Sequential是用列表类似的逻辑存储这些层的，保证了有序性）\n\n另外Sequential也是完全支持嵌套的，生成路径逻辑也是完全一样的（比如一个层的路径是0.2.1）块中加载块会保留路径结构\n\n这里我们在add.module中会指定加入的块名称（这里是block {i}），这个名称是有唯一性的，会覆盖（比如下面就重新写了一遍block 0，就出现了覆盖）\n\n不过add.module中指定的文本地址和隐性的数字地址是同时存在的，事实上，在检索时，用列表格式只能以数字作为路径检索，不过文本路径检索同样是可以用getattr实现\n\n我们可以调用内置的apply函数来遍历每一个module类实体，从而进行参数的初始化调整\n\n同样我们可以实现自定义的权重分配\n\n（这里abs()函数调用时执行的是逐元素操作）\n以及我们可以直接对单个元素进行操作\n\n同时我们可以用类似浅拷贝的思路实现参数绑定\n\n在这个例子中，两个参数共享同样的地址，所以说在梯度更新时，这个参数也会被更新两遍（分别来自两个层计算出来的梯度），直观上理解为这两个梯度的叠加作用在了这组参数上\n","categories":["Notes","d2l"],"tags":["Study"]},{"title":"Notes on Section 5.3","url":"/2026/01/05/Notes-on-Section-5-3/","content":"对于MXNET&#x2F; TENSORFLOW 两个框架来说，其稠密层（Dense，等效于线性层）的初始化是不必须包括输入结构的，也就是说只需要规定该层的输出形状，而模型会自动在参数进入后补充所需要的权重和输入结构信息\n\n（以上是一个还没有填入权重的网络框架）\n提供输入参数前\n\n提供输入参数后\n\n这个过程称为延后初始化\n延后初始化有利于自动匹配层之间的结构，可以简化架构时的考量，特别是特殊类型层向稠密层的过渡\n另外PyTorch也有差不多的nn.LazyLinear（），效果类似\n","categories":["Notes","d2l"],"tags":["Study"]},{"title":"Notes on Section 5.4","url":"/2026/01/05/Notes-on-Section-5-4/","content":"这一部分主要讲述构建自定义层\n以下分别是一个带参数的层和不带参数的层的样例\n1. 在__init__中规定需要的参数2. 在forward函数中规定计算方式\n\n\n\n","categories":["Notes","d2l"],"tags":["Study"]},{"title":"Notes on Section 5.5","url":"/2026/01/05/Notes-on-Section-5-5/","content":"这一部分主要讲的是读写文件，主要是对于特定的网络层或者是训练参数，我们可能需要封装或者是转移（或者只是中途存储）。这些操作提高了数据的安全性和灵活性\n我们可以用save和load从内存中存入和取出tensor\n\n\nX不一定要求是张量，也可是字符串向张量映射的字典类结构\n\n\n接下来我们可以把这些参数传入另一个模型\n\n注意的是，当我们使用load_state_dict时，我们需要参数的结构和模型是完全一致的，甚至来说路径都是一致的（可以参考5.1中路径的一些表述）\n不过我们不一定要整个的迁移数据，我们可以将模型分块，从块导出数据，再导入另一个等效块中，这使得卷积层等一些较为固定的层的数据迁移便利化\n相同的参数的模型给出的结果当然是一样的，没有传入参数的模型自动初始化，给出的输出是不同的（参考下图，clone_diff没有load参数）\n\n","categories":["Notes","d2l"],"tags":["Study"]},{"title":"Notes on Section 5.6","url":"/2026/01/06/Notes-on-Section-5-6/","content":"此时此刻，作者的laptop上既没有一个gpu，也没有两个gpu，于是本章如此潦草作罢\n这一节讲的是GPU在计算上的应用，以及如何优化数据的传递以利用并行运算的优势\n系统创建张量的时候，默认会存储在cpu上，我们可以通过复制传递这个信息\n我们可以指定张量创建在gpu上\n\n这里的try_gpu主要是为了防止没有gpu（说谁呢）\n\n所以核心其实是在创建的时候传入一个设备参数，这里是torch.device(f’cuda:{i}’)\n以下的一些代码可以用于查询现有设备下的gpu数量\n\n同时神经网络模型也接受不同的设备分配部署\n\n比如这样子可以将我们现在定义的net层部署到gpu\n这样的部署会将权重&#x2F;偏置等一些参数转移到gpu上，cpu仅保留句柄\n","categories":["Notes","d2l"],"tags":["Study"]},{"title":"Notes on Section 6.3","url":"/2026/01/07/Notes-on-Section-6-3/","content":"本节我们将介绍填充（padding）和步幅（stride）\n假设以下情景： 有时，在应用了连续的卷积之后，我们最终得到的输出远小于输入大小。这是由于卷积核的宽度和高度通常大于1所导致的。比如，一个240*240像素的图像，经过层的卷积后，将减少到200*200像素。如此一来，原始图像的边界丢失了许多有用信息。而填充是解决此问题最有效的方法； 有时，我们可能希望大幅降低图像的宽度和高度。例如，如果我们发现原始的输入分辨率十分冗余。步幅则可以在这类情况下提供帮助。\n通过如下实现，注意到当我们给kernel_size&#x2F;padding&#x2F;stride填入一个参数的时候，默认会广播成正方形（即x-&gt;x*x ），如果给两个参数，就是行&#x2F;列\n\n当我们希望一个奇数核对应图的每个位置都被作为中心计算时，padding&#x3D;(kernel_size-1)&#x2F;2\n默认条件下，卷积窗口每次滑动一个元素，不过我们可以调整这一距离来减少采样次数，提高训练效率。\n\n综上，填充和步幅可用于有效的调整数据的维度\n","categories":["Notes","d2l"],"tags":["Study"]},{"title":"Notes on Section 6.1","url":"/2026/01/06/Notes-on-Section-6-1/","content":"这一节开始将进入卷积层的一些相关知识\n“我们之前讨论的多层感知机十分适合处理表格数据，其中行对应样本，列对应特征。 对于表格数据，我们寻找的模式可能涉及特征之间的交互，但是我们不能预先假设任何与特征交互相关的先验结构。 此时，多层感知机可能是最好的选择，然而对于高维感知数据，这种缺少结构的网络可能会变得不实用。”\n这里的不实用，主要指的是稠密层不引入任何的先验，也就是其会尝试任意两个特征之间的关系，不管其在空间上相距多远。在表格中这是有意义的，因为表格的空间结构并不代表其中任意两个特征之间是否存在强力的连接。但是在图像的二维处理中，最左上角的像素和最左下角的像素对于整个图像的含义的整体性关联可能就没有这么强了。\n所以说全连接层训练模型的弊端就是其可能相对依赖其中某一些特征，而不是特征之间的关系，如果这个特征改变了，那么模型就可能给出错误的回答\n另一个问题是对于二维的图像数据，参数量是平方增长的，这使得使用该模型对大图像的识别变得不太可能\n卷积神经网络（convolutional neural networks，CNN）是机器学习利用自然图像中一些已知结构的创造性方法\n对于人来说，如果在已有的黑白图中拆掉一整根线，这个图可能就无法被正常识别了，但是随机的将10%的黑色像素涂白，这个图也许还可以识别的比较好。所以说我们这里可以同样的向模型引入这种先验——空间不变性（无论哪种方法找到这个物体，都应该和物体的位置无关）\n\n接下来我们进入数学的表达部分\n\nH_ij对应输出像素，X_ij对应输入像素，W是一个四维张量，把每个输入输出一一对应（这里的输入和输出预设形状一样）\n我们只需重新索引下标，使k&#x3D;i+a, l&#x3D;j+b，这时a，b就变成了相对位置（i，j）\n（这里的a，b可以正偏移或者负偏移，使得其效果和原先的k，l相同，我们也可以理解为是对于i，j偏移的补偿）\n这里我们为了比较方便的记录相对位移，允许V的负下标\n我们现在应用我们刚刚总结的平移不变性\n\n这里可以去掉u，V的关于i，j的下标指示，正是因为我们觉得这个部分的特征值是和位置无关的，也就是说H_ij不应该被其坐标位置的参数影响\n这就是卷积！（狗头）\nOk我们现在应该开始运用我们刚刚得到的另一个特质了，也就是局限性\n\n这里我们认为，只研究该像素周围的 (delta*2+1)^2的部分就ok了，这是因为我们认为相关信息是存在有效距离的\n“在深度学习研究社区中，被称为V卷积核（convolution kernel）或者滤波器（filter），亦或简单地称之为该卷积层的权重，通常该权重是可学习的参数”\n卷积层极大的简化了参数的开销，但是也带来了一些问题\n当出现特定符号需要出现在特定位置的时候，这种判断就会失效，比如说想训练一个观测表格打勾的模型，勾出现在正确框和错误框的语义是完全相反的，但是由于卷积层只关注局部形态，其可能没有办法处理这种情境。\n\n在数学上我们这样定义一个卷积操作，在这个算子中，f，g是两个函数，x是输入（也可以认为是在指定位置），然后我们在空间中遍历z，积分f（z）和g（x-z），其中x-z就是当前遍历到的z和我们指定位置x的相对位置，z+x-z&#x3D;x，即我们用这个位置修正代入g作为 f（z）的权重\n这里我们会发现数学上的卷积是有互换性的\n\n在模型中，我们对卷积离散化处理\n\n通过比对，我们可以发现6.1.6和6.1.3其实是等效的\n不过，在处理图像的时候，除了图像的位置信息，我们往往还会有颜色相关的通道强度信息，比如说三个通道&#x2F;三种原色（红色、绿色和蓝色），这时候我们的数据解构就发生了变化\n实际上一个像素就被三分成了三个有独立强度的值。比如一个1024*1024*3的结构，这时候我们需要改写一下卷积的表示方式\n\n这里新增了c，d分别用于连接输入的通道和输出的通道，这个部分中，层内的通道是全连接的，也就是说，每一个输入的通道值都会影响每一个输出的通道值，这也是为什么我们同时增加了c，d两个变量\n我们观测这样一个问题\n\n这里说的就是每个通道的采样都是独立的，然后内部通道是全连接的形式\n","categories":["Notes","d2l"],"tags":["Study"]},{"title":"Notes on Section 6.2","url":"/2026/01/07/Notes-on-Section-6-2/","content":"如何理解下面这个话\n\n在6.1中，我们考虑了数学定义的卷积，那时候我们提出了相对位置的概念，这时候我们要求卷积核翻转和输入进行元素相乘，但是这里实际上卷积核没有翻转，而是直接覆盖在输入上进行乘法。也就是说我们没有翻转这一步操作\n事实上，翻不翻转对于模型训练没有意义，因为权重本身就是需要训练的对象而且不可视，翻转和不翻转最后只会体现在权重的数据分布上，不会体现在模型的效果上，所以说我们合理简化了这个操作\n另外我们发现输出变小了，这是因为卷积核不能超出边界运算，这使得边界的信息没有完整的记录下来（这个问题可以通过合理用0拓展边界来解决）\n\n我们可以直观的定义一下卷积核，不过这里使用了python自带的for循环，自然速度是极慢的（更合理的是正交方向的flatten两个向量组并矩阵乘法）不过这个更直观\n\n以及其初始化\n\n“高度和宽度分别为h,w积核可以被称为h*w卷积或h*w卷积核。 我们也将带有h*w卷积核的卷积层称为h*w卷积层”\n以下我们进行一个小小的应用\n\n不过一个显而易见的事情是，以上卷积核只适合检测垂直边界，当边界转变为水平时，输出全为0，这个卷积核就失效了\n以上例子说明，手动设计卷积核（滤波器，filter）在多卷积层和多参数的情境下往往是不现实的，所以这里这里必须引入学习式的生成卷积核\n我们尝试训练上一个例子中的卷积核，最终我们确实得到了我们想要的结果，这说明卷积核的权重也是完全可以训练的（损失函数使用L2正则化）\n\n!!!atention!!!这里用的Conv2d并不是我们上面定义的Conv2D（case-sensitive），然后这里用的是python自己的二维卷积层，默认前面的两个值对应批次数量和通道数\n是的pytorch的顺序是（批量大小、通道、高度、宽度）\n但是tensorflow是（批量大小、高度、宽度、通道）\n两者存在一定区别\n这里的权重已经非常接近[1 , -1]\n\n这里解释了开头陈述的问题，不过关于这部分笔者在开头也有解释\n“在卷积神经网络中，对于某一层的任意元素x，其感受野（receptive field）是指在前向传播期间可能影响x计算的所有元素（来自所有先前层）。”\n","categories":["Notes","d2l"],"tags":["Study"]},{"title":"Notes on Section 6.4","url":"/2026/01/09/Notes-on-Section-6-4/","content":"这一节我们讨论的是多通道输入和多通道输出的相关处理\n先看输入，其实本质上和单通道是一样的，我们增加了维度信息，而相乘模式还是元素对对应位置相乘。不过需要注意的是，当我们改变了输入通道结构，卷积核的结构也要相应的改变\n\n以上我们讨论的限于单通道输出，但是在实际应用中，多通道输出是非常重要的\n“在最流行的神经网络架构中，随着神经网络层数的加深，我们常会增加输出通道的维数，通过减少空间分辨率以获得更大的通道深度。直观地说，我们可以将每个通道看作对不同特征的响应。”\n以下构建了一个三通道输出的卷积核（输入是双通道，2*2卷积核）\n\n1*1卷积层\n这样结构的卷积层，显然不能提取周围像素的特征，但是像素内的通道之间的计算依然存在\n这样一个层有如下的一些作用：\n灵活的升维与降维（减少&#x2F;增加参数量）\n跨通道的信息交互（无干扰）\n减少计算量\n搭配非线性激活函数使用（比如ReLU）\n以下是书中的描述\n\n","categories":["Notes","d2l"],"tags":["Study"]},{"title":"Notes on Section 7.1","url":"/2026/01/17/Notes-on-Section-7-1/","content":"我们先来看经典机器学习的流程\n\n\n但是这些算法都基于预设的特征，这导致了其局限性。进一步的，我们希望机器可以自己学习特征\n不过这里我们可以先引入一下硬件更迭对于接下来介绍的一些算法出现的影响\n“GPU可优化高吞吐量的4*4矩阵和向量乘法，从而服务于基本的图形任务。幸运的是，这些数学运算与卷积层的计算惊人地相似。”\n“那么GPU比CPU强在哪里呢？”\n“首先，我们深度理解一下中央处理器（Central Processing Unit，CPU）的核心。 CPU的每个核心都拥有高时钟频率的运行能力，和高达数MB的三级缓存（L3Cache）。 它们非常适合执行各种指令，具有分支预测器、深层流水线和其他使CPU能够运行各种程序的功能。 然而，这种明显的优势也是它的致命弱点：通用核心的制造成本非常高。 它们需要大量的芯片面积、复杂的支持结构（内存接口、内核之间的缓存逻辑、高速互连等等），而且它们在任何单个任务上的性能都相对较差。 现代笔记本电脑最多有4核，即使是高端服务器也很少超过64核，因为它们的性价比不高。”\n“相比于CPU，GPU由100-1000个小的处理单元组成（NVIDIA、ATI、ARM和其他芯片供应商之间的细节稍有不同），通常被分成更大的组（NVIDIA称之为warps）。 虽然每个GPU核心都相对较弱，有时甚至以低于1GHz的时钟频率运行，但庞大的核心数量使GPU比CPU快几个数量级”\n我们可以比较LeNet和AlexNet的架构\n\n可以说AlexNet是放大增强版，而且更多的卷积层来放大感受野对于分辨率更高的图像也是必要的举措\n（实现代码略）\n这里我们不禁思考一个问题，也就是AlexNet的泛化性能。因为AlexNet是针对ImageNet的100w张224*224的共1000类的照片作为数据参照设计的。\n1. 比如说用10*10或者10000*10000的图片作为AlexNet的训练集，那么效果是否下降2. 我们假设224*224还有100*100还有500*500分别作为训练集（测试集也是对应的分辨率），那是不是还是224*224的训练效果最好？也即，AlexNet是不是对于224*224图片分辨率设计的特化模型，而当分辨率偏移的时候训练的效果就会下降（即使不是下降很快）3. AlexNet的参数是对于分类数量已知的先验的情况下优化的吗，比如说训练集有10万的类目，或者只有10个类目，这个神经网络的质量是否会下降呢对于以上三个问题，答案都是True，这也展现了AlexNet作为早期CNN的局限性\n","categories":["Notes","d2l"],"tags":["Study"]},{"title":"Hello World","url":"/2026/01/05/hello-world/","content":"Welcome to Hexo! This is your very first post. Check documentation for more info. If you get any problems when using Hexo, you can find the answer in troubleshooting or you can ask me on GitHub.\nQuick StartCreate a new post$ hexo new &quot;My New Post&quot;\n\nMore info: Writing\nRun server$ hexo server\n\nMore info: Server\nGenerate static files$ hexo generate\n\nMore info: Generating\nDeploy to remote sites$ hexo deploy\n\nMore info: Deployment\n"},{"title":"Notes on Section 6.5","url":"/2026/01/10/Notes-on-Section-6-5/","content":"在处理图像信息的时候，我们希望如下的操作：\n“逐渐降低隐藏表示的空间分辨率、聚集信息，随着我们在神经网络中层叠的上升，每个神经元对其敏感的感受野（输入）就越大。”\n我们现在假想一个任务，比如说从一个图像中分辨出某一个事物，那么，我们的最后一层的神经元理应对整个输入具有全局敏感。同时降低出对于空间位置采样的敏感性\n这一节，我们将引入汇聚（pooling）层，也称为池化层\n其具有双重目的：降低卷积层对位置的敏感性，同时降低对空间降采样表示的敏感性。\n以下我们定义一个池化层\n\n这里我么可以发现池化层和卷积层的关联，首先就是都有一个类似卷积核的窗口（pool size）以及步长（stride），不过区别在于，池化层没有需要训练的参数而更像一个非线性函数，其运作逻辑在图中体现了两种，分别是取窗口内最大值和平均值\n\n从这个部分可以看出来，其运作逻辑和卷积层是非常类同的\n在接受多个通道的时候，池化层在每个输入通道上单独运算，所以说输入通道数和输出通道数也是相同的\n通过对池化层运作逻辑的研究，我们可以总结出池化层的一些功能：\n**取区域最值&#x2F;均值-平移不变性-**解决像素位移导致结果巨变的问题\n**尺寸下采样-增大感受野-**有利于整体观测\n**丢弃部分空间信息-抽象化与降噪-**提高模型的鲁棒性，减少计算负担\n","categories":["Notes","d2l"],"tags":["Study"]},{"title":"Notes on Section 6.6","url":"/2026/01/12/Notes-on-Section-6-6/","content":"这一节主要讲述的是LeNet的架构\n我们一fashion-mnis测试集作为数据，则对应的网络结构如下\n\n\n我们可以观察到，LeNet由两组“卷积+激活+池化”构成卷积块，然后通过展平层和全连接快相连\n“请注意，虽然ReLU和最大汇聚层更有效，但它们在20世纪90年代还没有出现。”\n在两个卷积层的作用下，模型的感受野变大，特征维度增加（通道），从而语义表达增强。\n在线性层处理的时候，经由400-120-84-10可以逐层归纳特征，（其中84与LeNet 当时使用了一种基于 ASCII 码字符位图 的设计有关），另外在原网络中最后一层存在一个高斯激活，在这里被省略了。\n","categories":["Notes","d2l"],"tags":["Study"]}]